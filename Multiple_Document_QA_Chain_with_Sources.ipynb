{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "i-d2cgoVknlx"
      },
      "outputs": [],
      "source": [
        "# https://www.youtube.com/watch?v=3yPBVii7Ct0&list=PLG-xg5pFIsQXSGzlrHvpqGH5EFGVBz2-3&index=4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RRYSu48huSUW"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip -q install langchain openai tiktoken chromadb pypdf"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BOlXEzQymaO8"
      },
      "source": [
        "# Setup OpenAI API Key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "us3F8ZKeRiz2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"KEY\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CWqW1_FNo_Yy"
      },
      "source": [
        "# Import required langchain modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qK1nY4PkKYGo"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader, PyPDFLoader, DirectoryLoader\n",
        "from langchain.chat_models import ChatOpenAI"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Baa7IVLYme1e"
      },
      "source": [
        "# Load and process multiple docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjCi220DnQ5V",
        "outputId": "01765fb8-f782-48f4-b08e-8b2a2b0ded2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zP47pc_OluK8"
      },
      "outputs": [],
      "source": [
        "# Load and process the text files\n",
        "\n",
        "# loader = DirectoryLoader('/content/drive/MyDrive/data/webpages/', glob=\"./*.txt\", loader_cls=TextLoader)\n",
        "loader = DirectoryLoader('/content/drive/MyDrive/data/pdfs/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2K9AtdoqomI5"
      },
      "outputs": [],
      "source": [
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZC5rDj95luK9"
      },
      "outputs": [],
      "source": [
        "#splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zusuPRhluK9",
        "outputId": "41e65960-716f-4714-bae3-cc1597df7b9c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1059"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SavfZwnnluK9",
        "outputId": "5ad99a6f-963e-404f-83fc-97e1b2fc0aa9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='looking for huge waves while some may just be looking for windless classic waves. \\nJust like surf check, ISPO will be providing', metadata={'source': '/content/drive/MyDrive/data/pdfs/coming-to-ucsd-guide.pdf', 'page': 1})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[3]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1SrnfdCmkIw"
      },
      "source": [
        "# Create DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vOdoEpNYluK9"
      },
      "outputs": [],
      "source": [
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = 'db'\n",
        "\n",
        "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "396RyNbS4EXx"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yljNYLtnmpki"
      },
      "source": [
        "# Create chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "F3vkSxxYKCZ9"
      },
      "outputs": [],
      "source": [
        "# Set up the turbo LLM\n",
        "turbo_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-3.5-turbo'\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PsR60NH5KCfj"
      },
      "outputs": [],
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Rhxrnb-jmsQ9"
      },
      "source": [
        "# Helper function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EBW_vWe82xy5"
      },
      "outputs": [],
      "source": [
        "import textwrap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "RWulTG0eKCfk"
      },
      "outputs": [],
      "source": [
        "# Text formatting for the response\n",
        "def process_llm_response(llm_response):\n",
        "    result = llm_response['result']\n",
        "    result = 'Response: ' + result\n",
        "    wrapped_result = textwrap.wrap(result, width=80)\n",
        "    for line in wrapped_result:\n",
        "        print(line)\n",
        "    print('\\nSources:')\n",
        "    for source in llm_response['source_documents']:\n",
        "        print(source.metadata['source'])\n",
        "    print('\\n\\n')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7fPl26c-TbWw"
      },
      "source": [
        "# Chat prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwyuhrpu5XqM",
        "outputId": "9a2331b4-1d50-4556-d82b-cc5565bce72a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Use the following pieces of context to answer the users question. \n",
            "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "----------------\n",
            "{context}\n"
          ]
        }
      ],
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[0].prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcWXvSCHRvHO",
        "outputId": "465065d8-3918-4312-8029-511a4c11cf04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{question}\n"
          ]
        }
      ],
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.messages[1].prompt.template)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DTwUnAT1mvOk"
      },
      "source": [
        "# Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b90dsaI104-K",
        "outputId": "fbfa46d0-179f-4c4e-df49-9dc58343aec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your query (or 'q' to quit): How should I prepare for my arrival at UCSD?\n",
            "Response: To prepare for your arrival at UCSD, you should visit\n",
            "iNewStudent.ucsd.edu. There, you can find valuable immigration information and\n",
            "easy checklists to help you prepare. Additionally, you should contact your\n",
            "department (if you are a graduate student) or college to get further assistance.\n",
            "It is important to have your immigration documents ready upon arrival.\n",
            "\n",
            "Sources:\n",
            "/content/drive/MyDrive/data/pdfs/ispo-welcome-guide.pdf\n",
            "/content/drive/MyDrive/data/pdfs/coming-to-ucsd-guide.pdf\n",
            "/content/drive/MyDrive/data/pdfs/ispo-welcome-guide.pdf\n",
            "\n",
            "\n",
            "\n",
            "Enter your query (or 'q' to quit): What all immigration documents do I need?\n",
            "Response: Based on the context provided, the immigration documents you need are:\n",
            "- I-94 Record - Original Passport - Social Security (SSN) Card or Social\n",
            "Security Denial (Form SSA-L676) - Residency documents (have to show your name\n",
            "and the address)  Please note that this list may not be exhaustive and you\n",
            "should consult with an immigration lawyer or the relevant government agency for\n",
            "more information.\n",
            "\n",
            "Sources:\n",
            "/content/drive/MyDrive/data/pdfs/ispo-welcome-guide.pdf\n",
            "/content/drive/MyDrive/data/pdfs/coming-to-ucsd-guide.pdf\n",
            "/content/drive/MyDrive/data/pdfs/coming-to-ucsd-guide.pdf\n",
            "\n",
            "\n",
            "\n",
            "Enter your query (or 'q' to quit): q\n"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    while True:\n",
        "        query = input(\"Enter your query (or 'q' to quit): \")\n",
        "        if query == 'q':\n",
        "            break\n",
        "        llm_response = qa_chain(query)\n",
        "        process_llm_response(llm_response)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNfa5tZQ7dQb"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1tu4cKUAq1is"
      },
      "source": [
        "# Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "IEnm9vxBq2g-"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-4UGYJzq33X",
        "outputId": "01c49dba-fa6b-49cf-cf9a-efa482768d90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader, UnstructuredFileLoader, PyPDFLoader, DirectoryLoader\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "loader = DirectoryLoader('/content/drive/MyDrive/data/pdfs/', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "\n",
        "documents = loader.load()\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "\n",
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = 'db'\n",
        "\n",
        "## here we are using OpenAI embeddings but in future we will swap out to local embeddings\n",
        "embedding = OpenAIEmbeddings()\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)\n",
        "\n",
        "\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "# Set up the turbo LLM\n",
        "turbo_llm = ChatOpenAI(\n",
        "    temperature=0,\n",
        "    model_name='gpt-3.5-turbo'\n",
        ")\n",
        "\n",
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=turbo_llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)\n",
        "\n",
        "## Cite sources\n",
        "def process_llm_response(llm_response):\n",
        "    print(llm_response['result'])\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])\n",
        "\n",
        "def app():\n",
        "    st.title(\"Chat UI\")\n",
        "    query = st.text_input(\"Enter your query\")\n",
        "    if query:\n",
        "        llm_response = qa_chain(query)\n",
        "        st.write(f\"Response: {llm_response['result']}\")\n",
        "        st.write(f\"\\n\\nSources:\")\n",
        "        for source in llm_response['source_documents']:\n",
        "            st.write(source.metadata)\n",
        "\n",
        "app()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "bKVIWvdXrVUy"
      },
      "outputs": [],
      "source": [
        "!streamlit run /content/app.py &>/content/url.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhnQBa7Yt85L",
        "outputId": "0486d72c-72f7-4f23-d3da-b1e05fd7a426"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.416s\n",
            "your url is: https://silver-olives-learn.loca.lt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Prf-R54_zgMm"
      },
      "outputs": [],
      "source": [
        "# Copy external URL (without the port) from url.txt and paste it on the URL shown above"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
